{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0978f5b2",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This notebook goes through the basic usage of the LLM endpoints provided by LLMariner.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- LLMariner needs to be installed. Please visit\n",
    "  the [documentation site](https://llmariner.readthedocs.io/en/latest/index.html) for the installation procedure.\n",
    "- This notebook uses the [OpenAI Python library](https://github.com/openai/openai-python). Please run\n",
    "  `pip install openai` to install it.\n",
    "- This notebook requires an API key. Please run `llma auth login` and `llma auth api-keys create <Name>` to create an API key.\n",
    "\n",
    "## Set up a Client\n",
    "\n",
    "The first step is to create an `OpenAI` client. You need to set `base_url` and `api_key`\n",
    "based on your configuration.\n",
    "\n",
    "The value of `base_url` points to the address of the LLMariner API endpoint.\n",
    "For example, the `base_rul` is set to `http://localhost:8080/v1` if you're accessing\n",
    "the endpoint running at your localhost with port 8080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"<Update this>\",\n",
    "  api_key=\"<Update this>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd073c6",
   "metadata": {},
   "source": [
    "You can also just call `client = OpenAI()` if you set the following environment variables:\n",
    "\n",
    "- `OPENAI_BASE_URL`: LLMariner API endpoint URL (e.g., `http://localhost:8080/v1`)\n",
    "- `OPENAI_API_KEY`: LLMariner API Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa59b17",
   "metadata": {},
   "source": [
    "## Find Installed LLM Models\n",
    "\n",
    "Let's first find LLM models that have been installed. You can use\n",
    "these models for chat completion, fine-tuning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3aca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(sorted(list(map(lambda m: m.id, models.data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66337d60",
   "metadata": {},
   "source": [
    "If you install LLMariner with the default configuration, you should see `google-gemma-2b-it-q4_0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6793b3f",
   "metadata": {},
   "source": [
    "## Run Chat Completion\n",
    "\n",
    "Let's test chat completion.\n",
    "\n",
    "You can use any models that are shown in the above input for chat completion. Here let's pick up `google-gemma-2b-it-q4_0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-gemma-2b-it-q4_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a34da2",
   "metadata": {},
   "source": [
    "You can run the following script to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd87c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is k8s?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "   print(response.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab1d0a",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) allows a chat completion to argument a prompt with data retrieved from a vector database.\n",
    "\n",
    "This section explain how RAG works in LLMariner.\n",
    "\n",
    "First try the following query without RAG. The output is a hallucinated one as the model doesn't have knowledge\n",
    "on LLMariner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e057cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"google-gemma-2b-it-q4_0\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is LLMariner?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "  print(response.choices[0].delta.content, end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe075de",
   "metadata": {},
   "source": [
    "Then create a vector store and create a document that describes LLMariner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"llm_mariner_overview.txt\"\n",
    "with open(filename, \"w\") as fp:\n",
    "  fp.write(\"LLMariner builds a software stack that provides LLM as a service. It provides the OpenAI-compatible API.\")\n",
    "file = client.files.create(\n",
    "  file=open(filename, \"rb\"),\n",
    "  purpose=\"assistants\",\n",
    ")\n",
    "print(\"Uploaded file. ID=%s\" % file.id)\n",
    "\n",
    "vs = client.vector_stores.create(\n",
    "  name='Test vector store',\n",
    ")\n",
    "print(\"Created vector store. ID=%s\" % vs.id)\n",
    "\n",
    "vfs = client.vector_stores.files.create(\n",
    "  vector_store_id=vs.id,\n",
    "  file_id=file.id,\n",
    ")\n",
    "print(\"Created vector store file. ID=%s\" % vfs.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a920e1a",
   "metadata": {},
   "source": [
    "Running the same prompt with RAG generates an output that uses the information retrieved from the vector store\n",
    "without hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df91d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"google-gemma-2b-it-q4_0\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is LLMariner?\"}\n",
    "  ],\n",
    "  tool_choice = {\n",
    "   \"choice\": \"auto\",\n",
    "   \"type\": \"function\",\n",
    "   \"function\": {\n",
    "     \"name\": \"rag\"\n",
    "   }\n",
    " },\n",
    " tools = [\n",
    "   {\n",
    "     \"type\": \"function\",\n",
    "     \"function\": {\n",
    "       \"name\": \"rag\",\n",
    "       \"parameters\": {\n",
    "         \"vector_store_name\": \"Test vector store\"\n",
    "       }\n",
    "     }\n",
    "   }\n",
    " ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "  print(response.choices[0].delta.content, end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28593337",
   "metadata": {},
   "source": [
    "Please note that this tutorial uses a small model to make it run with CPU, and there is no guarantee\n",
    "that the model answers to the question correctly. Please switch to other model and use GPU for real use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dbf2d0",
   "metadata": {},
   "source": [
    "## Fine-tune a Model\n",
    "\n",
    "Let's fine-tune a model so that we can get a better answer on the above question.\n",
    "\n",
    "For simplicity, we create training data that has the exact question and answer.\n",
    "The format of the dataset follows [OpenAI page](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {\n",
    "  \"What is the API endpoint for listing all models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"How can we list all models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"What's the API request for listing models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"Is there any way to list all models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"Can you show me how to list all models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"How can we list all models in LLMariner?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"What is the API endpoint for listing all jobs?\": \"GET request to /v1/fine-tuning/jobs. No parameter is needed.\",\n",
    "  \"How can we list all jobs?\": \"GET request to /v1/fine-tuning/jobs. No parameter is needed.\",\n",
    "  \"What is the API endpoint for creating a new job?\": \"POST request to /v1/fine-tuning/jobs.\",\n",
    "  \"What is the API endpoint for listing all uploaded files?\": \"GET request to /v1/files. No parameter is needed.\",\n",
    "  \"How can we list all files?\": \"GET request to /v1/files. No parameter is needed.\",\n",
    "}\n",
    "\n",
    "def format_datapoint(q, a):\n",
    "  prompt = \"You are an expert text to API endpoint translator. Users will ask you questions in English and you will generate an endpoint for LLMariner. %s\" % q\n",
    "  return \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"%s\"}, {\"role\": \"assistant\", \"content\": \"%s\"}]}\"\"\" % (prompt, a)\n",
    "\n",
    "training_filename = \"my_training_data.jsonl\"\n",
    "with open(training_filename, \"w\") as fp:\n",
    "  for q, a in training_data.items():\n",
    "    fp.write(\"%s\\n\" % format_datapoint(q, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8968196",
   "metadata": {},
   "source": [
    "Next upload the file to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "  file=open(training_filename, \"rb\"),\n",
    "  purpose=\"fine-tune\",\n",
    ")\n",
    "print(\"Uploaded file. ID=%s\" % file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96096d3",
   "metadata": {},
   "source": [
    "You can verify the update succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.files.list().data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713a83b",
   "metadata": {},
   "source": [
    "Then start a fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ce5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "  model=\"google-gemma-2b-it-q4_0\",\n",
    "  suffix=\"fine-tuning\",\n",
    "  training_file=file.id,\n",
    ")\n",
    "print(\"Created job. ID=%s\" % job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edd5e2",
   "metadata": {},
   "source": [
    "A pod is created in your Kubernetes cluster in a namespace where a project is associated. By default pods run in the `default` namespace.\n",
    "\n",
    "You can check the progress of the fine-tuning job by accessing the K8s cluster or run the CLI command (e.g., `llma fine-tuning jobs list`, `llma fine-tuning jobs logs <job-id>`).\n",
    "\n",
    "You will need to wait for several minutes for job completion.\n",
    "\n",
    "Once the job completes, you can check the generated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = client.fine_tuning.jobs.list().data[0].fine_tuned_model\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d0675",
   "metadata": {},
   "source": [
    " The model is also included in the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(map(lambda m: m.id, client.models.list().data))\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd7668",
   "metadata": {},
   "source": [
    "Then you can use that for the chat completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0494d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"You are an text to API endpoint translator. Users will ask you questions in English and you will generate an endpoint for LLMariner. What is the API endpoint for listing models?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "  print(response.choices[0].delta.content, end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b588e4f",
   "metadata": {},
   "source": [
    "This is based on a small set of training data. While the answer is still not perfect, you can see a different response.\n",
    "\n",
    "## Use Hugging Face Dataset For Fine-Tuning\n",
    "\n",
    "If you have access to a Hugging Face dataset, you can use that for fine-tuning. The basic flow is\n",
    "the same as the previous one, but the dataset is loaded from Hugging Face.\n",
    "\n",
    "Please note that this might not work if there is no sufficient GPU memory a node.\n",
    "\n",
    "The prerequisite is following:\n",
    "\n",
    "- Set `HUGGING_FACE_HUB_TOKEN` to your Hugging Face token\n",
    "- Install `datasets` by running `pip install datasets`\n",
    "\n",
    "Then run the following code to generate a training file and a validation file.\n",
    "This follows [this blog article](https://medium.com/the-ai-forum/instruction-fine-tuning-gemma-2b-on-medical-reasoning-and-convert-the-finetuned-model-into-gguf-844191f8d329) to\n",
    "generate a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    prefix_text = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "    # Samples with additional context into.\n",
    "    if data_point['input']:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n",
    "    return text\n",
    "\n",
    "def replace_special_chars(text):\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('\\\\', '')\n",
    "    return text\n",
    "\n",
    "def format_datapoint(data_point):\n",
    "    prompt = generate_prompt(data_point)\n",
    "    prompt = replace_special_chars(prompt)\n",
    "    output = replace_special_chars(data_point[\"output\"])\n",
    "    return \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"%s\"}, {\"role\": \"assistant\", \"content\": \"%s\"}]}\"\"\" % (prompt, output)\n",
    "\n",
    "def create_file(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "      for d in data:\n",
    "          f.write(\"%s\\n\" % format_datapoint(d))\n",
    "\n",
    "dataset = load_dataset(\"mamachang/medical-reasoning\")\n",
    "dataset = dataset.shuffle(seed=1234)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "create_file(dataset[\"train\"], \"training.jsonl\")\n",
    "create_file(dataset[\"test\"], \"validation.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e20b1",
   "metadata": {},
   "source": [
    "Then you can submit a fine-tuning job with the generating training file and validation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_filename = \"training.jsonl\"\n",
    "\n",
    "tfile = client.files.create(\n",
    "  file=open(training_filename, \"rb\"),\n",
    "  purpose=\"fine-tune\",\n",
    ")\n",
    "print(\"Uploaded file. ID=%s\" % tfile.id)\n",
    "\n",
    "validation_filename = \"validation.jsonl\"\n",
    "vfile = client.files.create(\n",
    "  file=open(validation_filename, \"rb\"),\n",
    "  purpose=\"fine-tune\",\n",
    ")\n",
    "print(\"Uploaded file. ID=%s\" % vfile.id)\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "  model=\"google-gemma-2b-it\",\n",
    "  suffix=\"fine-tuning\",\n",
    "  training_file=tfile.id,\n",
    "  validation_file=vfile.id,\n",
    ")\n",
    "print(\"Created job. ID=%s\" % job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300dc61",
   "metadata": {},
   "source": [
    "Once the job completes, you can try chat completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = client.fine_tuning.jobs.list().data[0].fine_tuned_model\n",
    "print(fine_tuned_model)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request. Please answer with one of the option in the bracket. Write reasoning in between <analysis></analysis>. Write answer in between <answer></answer>.here are the inputs:Q:A 34-year-old man presents to a clinic with complaints of abdominal discomfort and blood in the urine for 2 days. He has had similar abdominal discomfort during the past 5 years, although he does not remember passing blood in the urine. He has had hypertension for the past 2 years, for which he has been prescribed medication. There is no history of weight loss, skin rashes, joint pain, vomiting, change in bowel habits, and smoking. On physical examination, there are ballotable flank masses bilaterally. The bowel sounds are normal. Renal function tests are as follows:\\nUrea 50 mg/dL\\nCreatinine 1.4 mg/dL\\nProtein Negative\\nRBC Numerous\\nThe patient underwent ultrasonography of the abdomen, which revealed enlarged kidneys and multiple anechoic cysts with well-defined walls. A CT scan confirmed the presence of multiple cysts in the kidneys. What is the most likely diagnosis?? \\n{'A': 'Autosomal dominant polycystic kidney disease (ADPKD)', 'B': 'Autosomal recessive polycystic kidney disease (ARPKD)', 'C': 'Medullary cystic disease', 'D': 'Simple renal cysts', 'E': 'Acquired cystic kidney disease'}\"} ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "   print(response.choices[0].delta.content, end=\"\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
