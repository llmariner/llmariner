{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440fb5b3",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This notebook goes through the basic usage of the LLM endpoints provided by LLMariner.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- LLMariner needs to be installed. Please visit\n",
    "  the [documentation site](https://llmariner.readthedocs.io/en/latest/index.html) for the installation procedure.\n",
    "- This notebook uses the [OpenAI Python library](https://github.com/openai/openai-python). Please run\n",
    "  `pip install openai` to install it.\n",
    "- This notebook requires an API key. Please run `llma auth login` and `llma auth api-keys create <Name>` to create an API key.\n",
    "\n",
    "## Set up a Client\n",
    "\n",
    "The first step is to create an `OpenAI` client. You need to set `base_url` and `api_key`\n",
    "based on your configuration.\n",
    "\n",
    "The value of `base_url` points to the address of the LLMariner API endpoint.\n",
    "For example, the `base_rul` is set to `http://localhost:8080/v1` if you're accessing\n",
    "the endpoint running at your localhost with port 8080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfc905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"<Update this>\",\n",
    "  api_key=\"<Update this>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40258d78",
   "metadata": {},
   "source": [
    "You can also just call `client = OpenAI()` if you set the following environment variables:\n",
    "\n",
    "- `OPENAI_BASE_URL`: LLMariner API endpoint URL (e.g., `http://localhost:8080/v1`)\n",
    "- `OPENAI_API_KEY`: LLMariner API Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587f52d",
   "metadata": {},
   "source": [
    "## Find Installed LLM Models\n",
    "\n",
    "Let's first find LLM models that have been installed. You can use\n",
    "these models for chat completion, fine-tuning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(sorted(list(map(lambda m: m.id, models.data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c337e6",
   "metadata": {},
   "source": [
    "If you install LLMariner with the default configuration, you should see `google-gemma-2b-it-q4_0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab7864",
   "metadata": {},
   "source": [
    "## Run Chat Completion\n",
    "\n",
    "Let's test chat completion.\n",
    "\n",
    "You can use any models that are shown in the above input for chat completion. Here let's pick up `google-gemma-2b-it-q4_0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b421457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-gemma-2b-it-q4_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a9206",
   "metadata": {},
   "source": [
    "You can run the following script to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is k8s?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "   print(response.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20342503",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) allows a chat completion to argument a prompt with data retrieved from a vector database.\n",
    "\n",
    "This section explain how RAG works in LLMariner.\n",
    "\n",
    "First try the following query without RAG. The output is a hallucinated one as the model doesn't have knowledge\n",
    "on LLMariner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebdabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"google-gemma-2b-it-q4_0\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is LLMariner?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "  print(response.choices[0].delta.content, end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0c735",
   "metadata": {},
   "source": [
    "Then create a vector store and create a document that describes LLMariner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6da001",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"llm_mariner_overview.txt\"\n",
    "with open(filename, \"w\") as fp:\n",
    "  fp.write(\"LLMariner builds a software stack that provides LLM as a service. It provides the OpenAI-compatible API.\")\n",
    "file = client.files.create(\n",
    "  file=open(filename, \"rb\"),\n",
    "  purpose=\"assistants\",\n",
    ")\n",
    "print(\"Uploaded file. ID=%s\" % file.id)\n",
    "\n",
    "vs = client.vector_stores.create(\n",
    "  name='Test vector store',\n",
    ")\n",
    "print(\"Created vector store. ID=%s\" % vs.id)\n",
    "\n",
    "vfs = client.vector_stores.files.create(\n",
    "  vector_store_id=vs.id,\n",
    "  file_id=file.id,\n",
    ")\n",
    "print(\"Created vector store file. ID=%s\" % vfs.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a37f4b",
   "metadata": {},
   "source": [
    "Running the same prompt with RAG generates an output that uses the information retrieved from the vector store\n",
    "without hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"google-gemma-2b-it-q4_0\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is LLMariner?\"}\n",
    "  ],\n",
    "  tool_choice = {\n",
    "   \"choice\": \"auto\",\n",
    "   \"type\": \"function\",\n",
    "   \"function\": {\n",
    "     \"name\": \"rag\"\n",
    "   }\n",
    " },\n",
    " tools = [\n",
    "   {\n",
    "     \"type\": \"function\",\n",
    "     \"function\": {\n",
    "       \"name\": \"rag\",\n",
    "       \"parameters\": {\n",
    "         \"vector_store_name\": \"Test vector store\"\n",
    "       }\n",
    "     }\n",
    "   }\n",
    " ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "  print(response.choices[0].delta.content, end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee111f96",
   "metadata": {},
   "source": [
    "Please note that this tutorial uses a small model to make it run with CPU, and there is no guarantee\n",
    "that the model answers to the question correctly. Please switch to other model and use GPU for real use cases."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
