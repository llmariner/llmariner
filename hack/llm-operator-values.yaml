# This values file is for a dev env where kong, minio, and postgres are installed in the same k8s cluster.

global:
  llmOperatorBaseUrl: http://kong-proxy.kong/v1/

  database:
    host: postgres.postgres
    port: 5432
    username: ps_user
    ssl:
      mode: disable
    createDatabase: true

  databaseSecret:
    name: postgres
    key: password

  objectStore:
    s3:
      endpointUrl: http://minio.minio:9000
      region: dummy
      bucket: llm-operator

  awsSecret:
    name: aws
    accessKeyIdKey: accessKeyId
    secretAccessKeyKey: secretAccessKey

  ingress:
    ingressClassName: kong

  auth:
    # This works when a local env accesses the LLM Operator endpoint (= ingress controller)
    # with http://localhost:8080
    oidcIssuerUrl: http://localhost:8080/v1/dex


cluster-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


dex-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


file-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


inference-manager-engine:
  logLevel: 1
  replicaCount: 2
  model:
    default:
      runtimeName: ollama
      resources:
        limits:
          cpu: 0
          memory: 0
          # Do not allocate GPU to inference-manager-engine since g5.4xlarge has only one GPU,
          # and it is needed for the fine-tuning job
          nvidia.com/gpu: 0
        requests:
          cpu: 0
          memory: 0
    overrides:
      google/gemma-2b-it-q4_0:
        preloaded: true
      sentence-transformers/all-MiniLM-L6-v2-f16:
        preloaded: true
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


inference-manager-server:
  service:
    annotations:
      konghq.com/connect-timeout: "360000"
      konghq.com/read-timeout: "360000"
      konghq.com/write-timeout: "360000"
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


job-manager-dispatcher:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


job-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


model-manager-loader:
  baseModels:
  - google/gemma-2b-it-q4_0
  - sentence-transformers/all-MiniLM-L6-v2-f16
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


model-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


rbac-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


session-manager-agent:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


session-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


user-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


vector-store-manager-server:
  llmEngineAddr: ollama-sentence-transformers-all-minilm-l6-v2-f16:11434
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0
