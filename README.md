# LLM Operatror

LLM Operator converts your GPU clusters to a platform for generative AI workloads.

# Key Values

- *Provide LLM as a service.* LLM Operator builds a software stack that provides LLM as a service, including inference, fine-tuning, model management, and training data management.
- *Utilize GPU optimally.* LLM Operator provides auto-scaling of inference-workloads, efficient scheduling of fine-tuning batch jobs, GPU sharing, etc.

# Use Cases

- Develop LLM applications with the API that is compatible with [OpenAI-compatible API](https://platform.openai.com/docs/api-reference).
- Fine-tune models while keeping data safely and securely in your on-premise datacenter.
- Run fine-tuning jobs efficiently with guaranteed SLO and without interference with inference requests.

# Installation

Please visit [our documentation site](https://llm-operator.readthedocs.io/).

# High-level Architecture

![Architecture Diagram](docs/images/architecture_diagram.png)

# A Demo Video

Please see [the demo video](https://drive.google.com/file/d/1IIDytriu4Cl1O9Wo7fXzHkS1kbqJxfXO/view?usp=sharing).
