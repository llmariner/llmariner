# This values file is for a dev env where kong, minio, and postgres are installed in the same k8s cluster.

global:
  database:
    ssl:
      mode: disable

  awsSecret:
    name: aws

api-usage-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


cluster-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


dex-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


file-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


inference-manager-engine:
  logLevel: 1
  replicaCount: 2
  model:
    default:
      runtimeName: ollama
      resources:
        limits:
          cpu: 0
          memory: 0
          # Do not allocate GPU to inference-manager-engine since g5.4xlarge has only one GPU,
          # and it is needed for the fine-tuning job
          nvidia.com/gpu: 0
        requests:
          cpu: 0
          memory: 0
    overrides:
      google/gemma-2b-it-q4_0:
        preloaded: true
      sentence-transformers/all-MiniLM-L6-v2-f16:
        preloaded: true
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


inference-manager-server:
  service:
    annotations:
      konghq.com/connect-timeout: "360000"
      konghq.com/read-timeout: "360000"
      konghq.com/write-timeout: "360000"
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


job-manager-dispatcher:
  notebook:
    # Used to set the base URL of the API endpoint. Set this
    # to the URL that is reachable inside the K8s cluster.
    llmarinerBaseUrl: http://kong-proxy.kong/v1

  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


job-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


model-manager-loader:
  baseModels:
  - google/gemma-2b-it-q4_0
  - sentence-transformers/all-MiniLM-L6-v2-f16
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


model-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


rbac-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


session-manager-agent:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


session-manager-server:
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


user-manager-server:
  defaultApiKey:
    name: default-key
    secret: default-key-secret
    userId: admin@example.com
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0


vector-store-manager-server:
  llmEngineAddr: ollama-sentence-transformers-all-minilm-l6-v2-f16:11434
  # To fit in a single node
  resources:
    limits:
      cpu: 0
      memory: 0
    requests:
      cpu: 0
      memory: 0
